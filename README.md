
# LepiSense Input API

This project contains code for an API which provides a way to input data
generated by the LepiSense system. It is written in Python using the FastAPI
framework. Use it to upload images and manage deployment information.

It is created as a serverless application to be hosted on Amazon Web Services
(AWS) that you can deploy with the Serverless Application Model (SAM) Command
Line Interface (CLI).

The application creates several AWS resources, including a Lambda function and
an API Gateway. These resources are defined in the `template.yaml` file in this
project. You can update the template to add AWS resources through the same
deployment process that updates your application code.

It depends upon other resources that are created using the Cloud
Development Kit (CDK). Refer to https://github.com/NERC-CEH/lepisense-cdk and
follow the set up procedure that it describes. It feels like a nasty mash up
but it works.


## Development

To use the SAM CLI, you need the following tools installed.

* [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
* [SAM CLI](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)
* [Python 3](https://www.python.org/downloads/)
* [Docker](https://hub.docker.com/search/?type=edition&offering=community)
* [Git](https://git-scm.com/install/)


Build the application with the `sam build --use-container` command.

The SAM CLI installs dependencies defined in `app/requirements.txt`, creates a
deployment package, and saves it in the `.aws-sam/build` folder.

You might use `sam local start-api` to run the API locally on port 3000 but
you'd have to figure something out to put a local database in place. You would
also need to make the connection information available for the local database.
That would mean modifying secrets.py to load from file or environment when
there is no AWS secret to provide that information.

For the Lambda function to use the standard Python psycopg module for accessing
the database it has to be installed with the appropriate client libraries as 
they are not included in the operating system. For development,this has been
achieved by adding options in the requirements.txt
  
## Deployment

1. The database and S3 buckets are created on AWS by deploying the Stateless
Stack in the [CDK project](https://github.com/NERC-CEH/lepisense-cdk). 
2. Deploy this project to AWS, creating the input-api lambda function, by
   executing the following:
   ```
   aws sso login --profile <profile name>
   sam build --use-container
   sam deploy --profile <profile name> --config-env <stage>
   ```
   - You may be able to omit the profile option if you only have a single way to
   log in to AWS.
   - Replace `<stage>` by one of [dev|test|prod]

3. Deploy the Database Ingress Stack from the CDK project. This grants the
lambda function access to the database. 
4. In the [Secrets Manager console](console.aws.amazon.com/secretsmanager),
locate the `<stage>LepisenseApiJwtSecret` and reveal the secret values. You need
to generate a key by executing `openssl rand -hex 32` at the command line and
inserting the result as the key value.
5. Also in the Secrets Manager console, examine the secret
`<stage>LepisenseApiUserOneSecret`. You will need the username and password in a
moment.
6. Either from the output of step 2 or from the [API Gateway
console](https://console.aws.amazon.com/apigateway), copy the URL of the API.
Paste the URL in to your browser to see a web interface to the API.
7. Scroll down the page and locate the/database/reset endpoint. We need to 
trigger this to cause the database tables to be created. To log in, click the
padlock icon, enter the username and password noted in step 5, and click the 
Authorize button. After a brief pause another dialog appears. Click its Close 
button. To call the reset endpoint, click the Try It Out button and then the 
Execute button.

The API is now deployed.  

## Database Migrations

Alembic is installed for database migrations. The implementation is a bit odd
as there is no access to the database except via the lambda function and the 
API it provides.

First make any changes that are needed to app.sqlmodels and deploy the updated
lambda function to AWS. Use the `/database/revision` endpoint to then cause
alembic to autogenerate a revision file. Paste this in to the alembic/versions
folder with a file name in the format `<date as yyyymmdd>-<time as
hhmm>-<revision id>-<description>.py`

Rebuild and redeploy the function then use the `/database/upgrade` endpoint to 
cause the database to be updated.

On first run, with a new database, to ensure Alembic is in step with the 
version of the database created by FastAPI/SqlModel, use the `database/stamp`
endpoint.

## Use

A front end needs to be built for managing deployments. It should provide a
user-friendly way to enter operational data. In the interim, the web interface
can be used. The database/API is designed to be used as follows.

### Organisation
It is expected that use of LepiSense will be granted to organisations, such as 
UKCEH. There is a top level table where they are added. An abbreviated name for
the organisation is used as a key to the table.

### Country
It is expected that LepiSense will operate throughout the world so there is also
a top level Country table. The country will affect the models used and the times
at which things happen. A three-letter country code is used as a key to this
table.

### Network
A Network is a way to group Deployments belonging to an Organisation in a
Country. It is open to the user how many Networks they choose to create.

### Deployment
A Deployment is a record of a Device Type at a location, for example a moth
trap in your back garden.

### Device Type
While LepiSense has begun with moth traps, there is no reason not to use the 
system to deal with other instruments. The Device Type would determine how
data from that device would be handled.

### Device
Every single Device has a unique record and its use and history can be tracked.

### Deployment Device
To fulfil a Deployment of a Device Type, a specific Device will be used and this
is recorded in the Deployment Device table. If the Device needs servicing and
is replaced by a different Device then a new record is added to the Deployment
Device table. The dates when a Device in on a deployment are stored here.

### File
A Device will upload a File, hopefully in a regular, automated fashion. The file
is accompanied by the date and Device Id. From the Id, we know
where it is deployed. I have asked that the file be named according to the time
of capture in hhmmss format. Perhaps we will want to modify that and put the 
time as a parameter or the device id and date in the file name. 

### Inference
When a File is uploaded, it will create an Inference record if one does not
already exist. At this stage, it indicates the arrival of data which will be
processed by the inference system in due course. For moth traps there will be
one record per device per day and all files associated with that record will be
processed in a batch.

Because moth traps run overnight, their records span two dates. Therefore, a 
session date is used, which is the earlier date, but it includes files spanning
the two dates, from midday to midday. 

In the Device Type table, there is a night-session field which is set true for
moth traps and other such night-operating devices. Set it false for a device
that is day-operating.

It is expected that additional information will be added to this table about
when inferencing has been performed and with what models so that there is 
traceability regarding how outputs have been generated.

### Account
An account is for a user of the API. It is less likely to represent an
individual than an app which has its own user management system. Accounts are
tied to Organisations and restrict the account holder to only access the records 
of that Organisation. There are read, write and admin roles which limit which
end points the account can access. There is also a root role allowing access
to all Organisations.

At present, authentication and authorisation has not been fully implemented.